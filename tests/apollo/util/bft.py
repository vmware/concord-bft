# Concord
#
# Copyright (c) 2019 VMware, Inc. All Rights Reserved.
#
# This product is licensed to you under the Apache 2.0 license (the "License").
# You may not use this product except in compliance with the Apache 2.0 License.
#
# This product may include a number of subcomponents with separate copyright
# notices and license terms. Your use of these subcomponents is subject to the
# terms and conditions of the subcomponent's license, as noted in the LICENSE
# file.

# Add the pyclient directory to $PYTHONPATH

import sys

import os
import os.path
import shutil
import random
import subprocess
from collections import namedtuple
import tempfile
from functools import wraps
from datetime import datetime
import inspect

import trio

sys.path.append(os.path.abspath("../../util/pyclient"))

import bft_config
import bft_client
import bft_metrics_client
from enum import Enum
from util import bft_metrics, eliot_logging as log
from util.eliot_logging import log_call
from util import skvbc as kvbc
from util.bft_test_exceptions import AlreadyRunningError, AlreadyStoppedError, KeyExchangeError


TestConfig = namedtuple('TestConfig', [
    'n',
    'f',
    'c',
    'num_clients',
    'key_file_prefix',
    'start_replica_cmd',
    'stop_replica_cmd',
    'num_ro_replicas'
])

class ConsensusPathPrevalentResult(Enum):
   OK = 0
   TOO_FEW_REQUESTS_ON_EXPECTED_PATH = 1
   TOO_MANY_REQUESTS_ON_UNEXPECTED_PATH = 2

KEY_FILE_PREFIX = "replica_keys_"

@log_call(action_type="Test_Configs", include_args=[])
def interesting_configs(selected=None):
    if selected is None:
        selected=lambda *config: True

    bft_configs = [{'n': 6, 'f': 1, 'c': 1, 'num_clients': 30},
                   {'n': 7, 'f': 2, 'c': 0, 'num_clients': 30},
                   # {'n': 4, 'f': 1, 'c': 0, 'num_clients': 30},
                   # {'n': 9, 'f': 2, 'c': 1, 'num_clients': 30}
                   # {'n': 12, 'f': 3, 'c': 1, 'num_clients': 30}
                   ]

    selected_bft_configs = \
        [conf for conf in bft_configs
         if selected(conf['n'], conf['f'], conf['c'])]

    assert len(selected_bft_configs) > 0, "No eligible BFT configs"

    for config in selected_bft_configs:
        assert config['n'] == 3 * config['f'] + 2 * config['c'] + 1, \
            "Invariant breached. Expected: n = 3f + 2c + 1"

    return selected_bft_configs


def with_trio(async_fn):
    """ Decorator for running a coroutine (async_fn) with trio. """
    @wraps(async_fn)
    def trio_wrapper(*args, **kwargs):
        if "already_in_trio" in kwargs:
            kwargs.pop("already_in_trio")
            return async_fn(*args, **kwargs)
        else:
            return trio.run(async_fn, *args, **kwargs)

    return trio_wrapper

def with_constant_load(async_fn):
    """
    Runs the decorated async function in parallel with constant load,
    generated by a dedicated (reserved) client. In order to work
    the wrapper assumes a 'bft_network' will be created and passed
    as a keyword argument. The wrapper also creates a 'skvbc' object
    and a 'nursery' which it passes to the wrapped function.
    """
    @wraps(async_fn)
    async def wrapper(*args, **kwargs):
        if "bft_network" in kwargs:
            bft_network = kwargs.pop("bft_network")
            skvbc = kvbc.SimpleKVBCProtocol(bft_network)
            client = await bft_network.new_reserved_client()
            async with trio.open_nursery() as nursery:
                nursery.start_soon(skvbc.send_indefinite_write_requests, client, 1)  # send a request every second
                await async_fn(*args, **kwargs, bft_network=bft_network, skvbc=skvbc, nursery=nursery)
                nursery.cancel_scope.cancel()
    return wrapper

def with_bft_network(start_replica_cmd, selected_configs=None, num_clients=None, num_ro_replicas=0):
    """
    Runs the decorated async function for all selected BFT configs
    start_replica_cmd is a callback which is used to start a replica. It should have the following
    signature:
        def start_replica_cmd(builddir, replica_id)
    or
        def start_replica_cmd(builddir, replica_id, config)
    If you want the bft test network configuration to be passed to your callback you should add
    third parameter named 'config' (the exact name is important!).
    If you don't need this configuration - use two parameters callback with any names you want.
    """
    def decorator(async_fn):
        @wraps(async_fn)
        async def wrapper(*args, **kwargs):
            if "bft_network" in kwargs:
                bft_network = kwargs.pop("bft_network")
                bft_network.is_existing = True
                with log.start_task(action_type=async_fn.__name__):
                    await async_fn(*args, **kwargs, bft_network=bft_network)
            else:
                for bft_config in interesting_configs(selected_configs):

                    config = TestConfig(n=bft_config['n'],
                                        f=bft_config['f'],
                                        c=bft_config['c'],
                                        num_clients=bft_config['num_clients'] \
                                            if num_clients is None \
                                            else num_clients,
                                        key_file_prefix=KEY_FILE_PREFIX,
                                        start_replica_cmd=start_replica_cmd,
                                        stop_replica_cmd=None,
                                        num_ro_replicas=num_ro_replicas)
                    with BftTestNetwork.new(config) as bft_network:
                        storage_type = os.environ.get("STORAGE_TYPE")
                        bft_network.current_test = async_fn.__name__ + "_" + storage_type \
                                                                     + "_n=" + str(bft_config['n']) \
                                                                     + "_f=" + str(bft_config['f']) \
                                                                     + "_c=" + str(bft_config['c'])
                        with log.start_task(action_type=f"{bft_network.current_test}_num_clients={config.num_clients}"):
                            await async_fn(*args, **kwargs, bft_network=bft_network)
        return wrapper

    return decorator

MAX_MSG_SIZE = 64*1024 # 64k
REQ_TIMEOUT_MILLI = 5000
RETRY_TIMEOUT_MILLI = 250
METRICS_TIMEOUT_SEC = 5


# TODO: This is not generic, but is required for use by SimpleKVBC. In the
# future we will likely want to change how we determine the lengths of keys and
# values, make them parameterizable, or generate keys in the protocols rather
# than tester. For now, all keys and values must be 21 bytes.
KV_LEN = 21


class BftTestNetwork:
    """Encapsulates a BFT network instance for testing purposes"""

    def __enter__(self):
        """context manager method for 'with' statements"""
        return self

    def __exit__(self, *args):
        """context manager method for 'with' statements"""
        if not self.is_existing:
            for client in self.clients.values():
                client.__exit__()
            for client in self.reserved_clients.values():
                client.__exit__()
            self.metrics.__exit__()
            self.stop_all_replicas()
            os.chdir(self.origdir)
            shutil.rmtree(self.testdir, ignore_errors=True)

    def __init__(self, is_existing, origdir,
                 config, testdir, builddir, toolsdir,
                 procs, replicas, clients, metrics, client_factory):
        self.is_existing = is_existing
        self.origdir = origdir
        self.config = config
        self.testdir = testdir
        self.builddir = builddir
        self.toolsdir = toolsdir
        self.procs = procs
        self.replicas = replicas
        self.clients = clients
        self.metrics = metrics
        self.reserved_clients = {}
        if client_factory:
            self.client_factory = client_factory
        else:
            self.client_factory = self._create_new_udp_client
        self.open_fds = {}
        self.current_test = ""

    @classmethod
    def new(cls, config, client_factory=None):
        builddir = os.path.abspath("../../build")
        toolsdir = os.path.join(builddir, "tools")
        testdir = tempfile.mkdtemp()
        bft_network = cls(
            is_existing=False,
            origdir=os.getcwd(),
            config=config,
            testdir=testdir,
            builddir=builddir,
            toolsdir=toolsdir,
            procs={},
            replicas=[bft_config.Replica(i, "127.0.0.1", 3710 + 2*i, 4710 + 2*i)
                for i in range(0, config.n + config.num_ro_replicas)],
            clients = {},
            metrics = None,
            client_factory = client_factory
        )

        #copy loggging.properties file
        shutil.copy(os.path.abspath("../simpleKVBC/scripts/logging.properties"), testdir)

        log.log_message(message_type=f"Running test in {bft_network.testdir}")

        os.chdir(bft_network.testdir)
        bft_network._generate_crypto_keys()

        bft_network._init_metrics()
        bft_network._create_clients()

        return bft_network

    @classmethod
    def existing(cls, config, replicas, clients, client_factory=None):
        bft_network = cls(
            is_existing=True,
            origdir=None,
            config=config,
            testdir=None,
            builddir=None,
            toolsdir=None,
            procs={r.id: r for r in replicas},
            replicas=replicas,
            clients={i: clients[i] for i in range(len(clients))},
            metrics=None,
            client_factory=client_factory
        )

        bft_network._init_metrics()
        return bft_network

    def change_configuration(self, config):
        """
        When changing an existing bft-network, we would want to change only its configuration related parts
        such as: n,f,c and the parts that are affected by this change (keys, and clients)
        We don't want to remove the logs, db and other state related components
        """
        # We cannot change anything if there are running replicas
        assert(len(self.procs) == 0)

        # remove all existing clients
        for client in self.clients.values():
            client.__exit__()
        for client in self.reserved_clients.values():
            client.__exit__()
        self.metrics.__exit__()
        self.clients = {}

        # set the new configuration and init the network
        self.config = config
        self.replicas = [bft_config.Replica(i, "127.0.0.1", 3710 + 2 * i, 4710 + 2 * i)
                    for i in range(0, config.n + config.num_ro_replicas)]

        self._generate_crypto_keys()

        self._init_metrics()
        self._create_clients()


    def _generate_crypto_keys(self):
        keygen = os.path.join(self.toolsdir, "GenerateConcordKeys")
        args = [keygen, "-n", str(self.config.n), "-f", str(self.config.f)]
        if self.config.num_ro_replicas > 0:
            args.extend(["-r", str(self.config.num_ro_replicas)])
        args.extend(["-o", self.config.key_file_prefix])
        subprocess.run(args, check=True)

    def _create_clients(self):
        for client_id in range(self.config.n + self.config.num_ro_replicas,
                               self.config.num_clients+self.config.n + self.config.num_ro_replicas):
            self.clients[client_id] = self.client_factory(client_id)

    def _create_new_udp_client(self, client_id):
        config = self._bft_config(client_id)
        return bft_client.UdpClient(config, self.replicas)

    async def new_client(self):
        client_id = max(self.clients.keys() | self.reserved_clients.keys()) + 1
        client = self.client_factory(client_id)
        self.clients[client_id] = client
        return client

    async def new_reserved_client(self):
        reserved_client_id = max(self.clients.keys() | self.reserved_clients.keys()) + 1
        reserved_client = self.client_factory(reserved_client_id)
        self.reserved_clients[reserved_client_id] = reserved_client
        return reserved_client

    def _bft_config(self, client_id):
        return bft_config.Config(client_id,
                                 self.config.f,
                                 self.config.c,
                                 MAX_MSG_SIZE,
                                 REQ_TIMEOUT_MILLI,
                                 RETRY_TIMEOUT_MILLI)

    def _init_metrics(self):
        metric_clients = {}
        for r in self.replicas:
            metric_clients[r.id] = bft_metrics_client.MetricsClient(r)
        self.metrics = bft_metrics.BftMetrics(metric_clients)

    def random_client(self):
        return random.choice(list(self.clients.values()))

    def random_clients(self, max_clients):
        return set(random.choices(list(self.clients.values()), k=max_clients))

    def start_replica_cmd(self, replica_id):
        """
        Returns command line to start replica with the given id
        If the callback accepts three parameters and one of them
        is named 'config' - pass the network configuration too.
        """
        with log.start_action(action_type="start_replica_cmd"):
            start_replica_fn_args = inspect.getfullargspec(self.config.start_replica_cmd).args
            if "config" in start_replica_fn_args and len(start_replica_fn_args) == 3:
                return self.config.start_replica_cmd(self.builddir, replica_id, self.config)
            else:
                return self.config.start_replica_cmd(self.builddir, replica_id)

    def stop_replica_cmd(self, replica_id):
        """
        Returns command line to stop a replica with the given id
        """
        return self.config.stop_replica_cmd(replica_id)

    def start_all_replicas(self):
        with log.start_action(action_type="start_all_replicas"):
            for i in range(0, self.config.n):
                try:
                    self.start_replica(i)
                except AlreadyRunningError:
                    if not self.is_existing:
                        raise

            assert len(self.procs) == self.config.n

    def stop_all_replicas(self):
        """ Stop all running replicas"""
        [self.stop_replica(i) for i in self.get_live_replicas()]
        assert len(self.procs) == 0

    def start_replicas(self, replicas):
        """
        Start from list "replicas"
        """
        [self.start_replica(r) for r in replicas]

    def stop_replicas(self, replicas):
        """
        Start from list "replicas"
        """
        for r in replicas:
            self.stop_replica(r)

    def start_replica(self, replica_id):
        """
        Start a replica if it isn't already started.
        Otherwise raise an AlreadyStoppedError.
        """
        with log.start_action(action_type="start_replica"):
            stdout_file = None
            stderr_file = None

            if os.environ.get('KEEP_APOLLO_LOGS', "").lower() in ["true", "on"]:
                test_name = os.environ.get('TEST_NAME')

                if not test_name:
                    now = datetime.now().strftime("%y-%m-%d_%H:%M:%S")
                    test_name = f"{now}_{self.current_test}"

                test_dir = f"{self.builddir}/tests/apollo/logs/{test_name}/{self.current_test}/"
                test_log = f"{test_dir}stdout_{replica_id}.log"

                os.makedirs(test_dir, exist_ok=True)

                stdout_file = open(test_log, 'w+')
                stderr_file = open(test_log, 'w+')

                stdout_file.write("############################################\n")
                stdout_file.flush()
                stderr_file.write("############################################\n")
                stderr_file.flush()

                self.open_fds[replica_id] = (stdout_file, stderr_file)

            if replica_id in self.procs:
                raise AlreadyRunningError(replica_id)

            if self.is_existing and self.config.stop_replica_cmd is not None:
                self.procs[replica_id] = self._start_external_replica(replica_id)
            else:
                self.procs[replica_id] = subprocess.Popen(
                                            self.start_replica_cmd(replica_id),
                                            stdout=stdout_file,
                                            stderr=stderr_file,
                                            close_fds=True)

    def _start_external_replica(self, replica_id):
        with log.start_action(action_type="_start_external_replica"):
            subprocess.run(
                self.start_replica_cmd(replica_id),
                check=True
            )

            return self.replicas[replica_id]

    def stop_replica(self, replica_id):
        """
        Stop a replica if it is running.
        Otherwise raise an AlreadyStoppedError.
        """
        with log.start_action(action_type="stop_replica"):
            if replica_id not in self.procs.keys():
                raise AlreadyStoppedError(replica_id)

            if self.is_existing and self.config.stop_replica_cmd is not None:
                self._stop_external_replica(replica_id)
            else:
                p = self.procs[replica_id]
                if os.environ.get('GRACEFUL_SHUTDOWN', "").lower() in set(["true", "on"]):
                    p.terminate()
                else:
                    p.kill()
                for fd in self.open_fds.get(replica_id, ()):
                    fd.close()
                p.wait()

            del self.procs[replica_id]

    def _stop_external_replica(self, replica_id):
        with log.start_action(action_type="_stop_external_replica"):
            subprocess.run(
                self.stop_replica_cmd(replica_id),
                check=True
            )

    def all_replicas(self, without=None):
        """
        Returns a list of all replicas excluding the "without" set
        """
        if without is None:
            without = set()

        return list(set(range(0, self.config.n)) - without)

    def random_set_of_replicas(self, size, without=None):
        if without is None:
            without = set()
        random_replicas = set()
        for _ in range(size):
            exclude_replicas = random_replicas | without
            random_replicas.add(random.choice(self.all_replicas(without=exclude_replicas)))
        return random_replicas

    def get_live_replicas(self):
        """
        Returns the id-s of all live replicas
        """
        return list(self.procs.keys())

    async def get_current_primary(self):
        """
        Returns the current primary replica id
        """
        with log.start_action(action_type="get_current_primary"):
            current_primary = await self.get_current_view()
            return current_primary % self.config.n

    async def get_current_view(self):
        """
        Returns the current view number
        """
        with log.start_action(action_type="get_current_view"):
            live_replica = random.choice(self.get_live_replicas())
            current_view = await self.wait_for_view(
                replica_id=live_replica, expected=None)

            return current_view


    async def get_metric(self, replica_id, bft_network, mtype, mname):
        with trio.fail_after(seconds=30):
            while True:
                with trio.move_on_after(seconds=1):
                    try:
                        key = ['replica', mtype, mname]
                        value = await bft_network.metrics.get(replica_id, *key)
                    except KeyError:
                        # metrics not yet available, continue looping
                        log.log_message(message_type=f"KeyError! '{mname}' not yet available.")
                    else:
                        return value

    async def wait_for_view(self, replica_id, expected=None,
                            err_msg="Expected view not reached"):
        """
        Waits for a view that matches the "expected" predicate,
        and returns the corresponding view number.

        If the "expected" predicate is not provided,
        returns the current view number.

        In case of a timeout, fails with the provided err_msg
        """
        with log.start_action(action_type="wait_for_view") as action:
            if expected is None:
                expected = lambda _: True

            matching_view = None
            nb_replicas_in_matching_view = 0
            try:
                matching_view = await self._wait_for_matching_agreed_view(replica_id, expected)
                action.log(message_type=f'Matching view #{matching_view} has been agreed among replicas.')

                nb_replicas_in_matching_view = await self._wait_for_active_view(matching_view)
                action.log(f'View #{matching_view} is active on '
                      f'{nb_replicas_in_matching_view} replicas '
                      f'({nb_replicas_in_matching_view} >= n-f = {self.config.n - self.config.f}).')

                return matching_view
            except trio.TooSlowError:
                assert False, err_msg + \
                              f'(matchingView={matching_view} ' \
                              f'replicasInMatchingView={nb_replicas_in_matching_view})'

    async def _wait_for_matching_agreed_view(self, replica_id, expected):
        """
        Wait for the last agreed view to match the "expected" predicate
        """
        with log.start_action(action_type="_wait_for_matching_agreed_view"):
            last_agreed_view = None
            with trio.fail_after(seconds=30):
                while True:
                    try:
                        with trio.move_on_after(seconds=1):
                            key = ['replica', 'Gauges', 'lastAgreedView']
                            view = await self.metrics.get(replica_id, *key)
                            if expected(view):
                                last_agreed_view = view
                                break
                    except KeyError:
                        # metrics not yet available, continue looping
                        continue
            return last_agreed_view

    async def _wait_for_active_view(self, view):
        """
        Wait for a view to become active on enough (n-f) replicas
        """
        with log.start_action(action_type="_wait_for_active_view"):
            with trio.fail_after(seconds=30):
                while True:
                    nb_replicas_in_view = await self._count_replicas_in_view(view)

                    # wait for n-f = 2f+2c+1 replicas to be in the expected view
                    if nb_replicas_in_view >= 2 * self.config.f + 2 * self.config.c + 1:
                        break
            return nb_replicas_in_view

    async def _count_replicas_in_view(self, view):
        """
        Count the number of replicas that have activated a given view
        """
        with log.start_action(action_type="_count_replicas_in_view"):
            nb_replicas_in_view = 0

            async def count_if_replica_in_view(r, expected_view):
                """
                A closure that allows concurrent counting of replicas
                that have activated a given view.
                """
                with log.start_action(action_type="count_if_replica_in_view"):
                    nonlocal nb_replicas_in_view

                    key = ['replica', 'Gauges', 'currentActiveView']

                    with trio.move_on_after(seconds=5):
                        while True:
                            with trio.move_on_after(seconds=1):
                                try:
                                    replica_view = await self.metrics.get(r, *key)
                                    if replica_view == expected_view:
                                        nb_replicas_in_view += 1
                                except KeyError:
                                    # metrics not yet available, continue looping
                                    continue
                                else:
                                    break

        async with trio.open_nursery() as nursery:
            for r in self.get_live_replicas():
                nursery.start_soon(
                    count_if_replica_in_view, r, view)
        return nb_replicas_in_view

    async def force_quorum_including_replica(self, replica_id):
        """
        Bring down a sufficient number of replicas (excluding the primary),
        so that the remaining replicas form a quorum that includes replica_id
        """
        with log.start_action(action_type="force_quorum_including_replica") as action:
            assert len(self.procs) >= 2 * self.config.f + self.config.c + 1
            primary = await self.get_current_primary()
            self.stop_replicas(self.random_set_of_replicas(
                len(self.procs) - (2 * self.config.f + self.config.c + 1), without={primary, replica_id}))

    async def wait_for_fetching_state(self, replica_id):
        """
        Check metrics on fetching replica to see if the replica is in a
        fetching state

        Returns the current source replica for state transfer.
        """
        with log.start_action(action_type="wait_for_fetching_state"):
            with trio.fail_after(10): # seconds
                while True:
                    with trio.move_on_after(.5): # seconds
                        is_fetching = await self.is_fetching(replica_id)
                        source_replica_id = await self.source_replica(replica_id)
                        if is_fetching:
                            return source_replica_id

    async def is_fetching(self, replica_id):
        """Return whether the current replica is fetching state"""
        key = ['bc_state_transfer', 'Statuses', 'fetching_state']
        state = await self.metrics.get(replica_id, *key)
        return state != "NotFetching"

    async def source_replica(self, replica_id):
        """Return whether the current replica has a source replica already set"""
        with log.start_action(action_type="source_replica"):
            key = ['bc_state_transfer', 'Gauges', 'current_source_replica']
            source_replica_id = await self.metrics.get(replica_id, *key)

            return source_replica_id

    async def wait_for_state_transfer_to_start(self):
        """
        Retry checking every .5 seconds until state transfer starts at least one
        node. Stop trying, and fail the test after 30 seconds.
        """
        with log.start_action(action_type="wait_for_state_transfer_to_start"):
            with trio.fail_after(30): # seconds
                async with trio.open_nursery() as nursery:
                    for replica in self.replicas:
                        nursery.start_soon(self._wait_to_receive_st_msgs,
                                           replica,
                                           nursery.cancel_scope)

    async def _wait_to_receive_st_msgs(self, replica, cancel_scope):
        """
        Check metrics to see if state transfer started. If so cancel the
        concurrent coroutines in the request scope.
        """
        with log.start_action(action_type="_wait_to_receive_st_msgs"):
            while True:
                with trio.move_on_after(.5): # seconds
                    try:
                        key = ['replica', 'Counters', 'receivedStateTransferMsgs']
                        n = await self.metrics.get(replica.id, *key)
                        if n > 0:
                            cancel_scope.cancel()
                    except KeyError:
                        continue # metrics not yet available, continue looping

    async def wait_for_state_transfer_to_stop(
            self,
            up_to_date_node,
            stale_node,
            stop_on_stable_seq_num=False):
        with log.start_action(action_type="wait_for_state_transfer_to_stop") as action:
            with trio.fail_after(30): # seconds
                # Get the lastExecutedSeqNumber from a started node
                if stop_on_stable_seq_num:
                    key = ['replica', 'Gauges', 'lastStableSeqNum']
                else:
                    key = ['replica', 'Gauges', 'lastExecutedSeqNum']
                expected_seq_num = await self.metrics.get(up_to_date_node, *key)
                last_n = -1
                while True:
                    with trio.move_on_after(.5): # seconds
                        metrics = await self.metrics.get_all(stale_node)
                        try:
                            n = self.metrics.get_local(metrics, *key)
                        except KeyError:
                            # ignore - the metric will eventually become available
                            pass
                        else:
                            # Debugging
                            if n != last_n:
                                last_n = n
                                checkpoint = ['bc_state_transfer',
                                              'Gauges',
                                              'last_stored_checkpoint']
                                on_transferring_complete = ['bc_state_transfer',
                                                            'Counters',
                                                            'on_transferring_complete']
                                action.log(message_type="wait_for_st_to_stop: expected_seq_num={} "
                                      "last_stored_checkpoint={} "
                                      "on_transferring_complete_count={}".format(
                                            n,
                                            self.metrics.get_local(metrics, *checkpoint),
                                            self.metrics.get_local(metrics,
                                                *on_transferring_complete)))
                            # Exit condition
                            if n >= expected_seq_num:
                                return

    async def wait_for_replicas_to_checkpoint(self, replica_ids, expected_checkpoint_num):
        """
        Wait for every replica in `replicas` to take a checkpoint.
        Check every .5 seconds and give fail after 30 seconds.
        """
        with log.start_action(action_type="wait_for_replicas_to_checkpoint"):
            with trio.fail_after(30): # seconds
                async with trio.open_nursery() as nursery:
                    for replica_id in replica_ids:
                        nursery.start_soon(self.wait_for_checkpoint, replica_id, expected_checkpoint_num)

    async def wait_for_checkpoint(self, replica_id, expected_checkpoint_num=None):
        """
        Wait for a single replica to reach the expected_checkpoint_num.
        If none is provided, return the last stored checkpoint.
        """
        with log.start_action(action_type="wait_for_checkpoint"):
            key = ['bc_state_transfer', 'Gauges', 'last_stored_checkpoint']
            if expected_checkpoint_num is None:
                expected_checkpoint_num = lambda _: True
            with trio.fail_after(30):
                while True:
                    with trio.move_on_after(.5): # seconds
                        try:
                            last_stored_checkpoint = await self.metrics.get(replica_id, *key)
                        except KeyError:
                            continue
                        else:
                            if expected_checkpoint_num(last_stored_checkpoint):
                                return last_stored_checkpoint

    async def wait_for_fast_path_to_be_prevalent(self, run_ops, threshold, replica_id=0):
        await self._wait_for_consensus_path_to_be_prevalent(
            fast=True, run_ops=run_ops, threshold=threshold, replica_id=replica_id)

    async def wait_for_slow_path_to_be_prevalent(self, run_ops, threshold, replica_id=0):
        await self._wait_for_consensus_path_to_be_prevalent(
            fast=False, run_ops=run_ops, threshold=threshold, replica_id=replica_id)

    async def _wait_for_consensus_path_to_be_prevalent(self, fast, run_ops, threshold, replica_id=0, timeout=90):
        """
        Waits until at least threshold operations are being executed in the selected path.
          run_ops: lambda that executes tracker.run_concurrent_ops or creates requests in some other way
          threshold: minimum number of requests that have to be executed in the correct path
        run_ops should produce at least threshold executions
        """
        with log.start_action(action_type="wait_for_%s_path_to_be_prevalent" % ("fast" if fast else "slow")):
            with trio.fail_after(seconds=timeout):
                done = False
                while not done:
                    #initial state to ensure requests are sent
                    res = ConsensusPathPrevalentResult.TOO_MANY_REQUESTS_ON_UNEXPECTED_PATH
                    nb_fast_paths_to_ignore = 0
                    nb_slow_paths_to_ignore = 0
                    with trio.move_on_after(seconds=15):  #retry timeout for not enough right path requests
                        while not done:
                            if res == ConsensusPathPrevalentResult.TOO_MANY_REQUESTS_ON_UNEXPECTED_PATH:
                                # we need to reset the counters
                                log.log_message("run_ops")
                                nb_fast_paths_to_ignore = await self.num_of_fast_path_requests(replica_id)
                                nb_slow_paths_to_ignore = await self.num_of_slow_path_requests(replica_id)
                                if run_ops:
                                    await run_ops()
                            res = await self._consensus_path_prevalent(
                                    fast,
                                    nb_fast_paths_to_ignore + ((threshold - 1) if fast else 0),
                                    nb_slow_paths_to_ignore + ((threshold - 1) if not fast else 0),
                                    replica_id)
                            if res == ConsensusPathPrevalentResult.OK:
                                # the selected path is prevalent - done.
                                log.log_message("done")
                                done = True
                                break
                            if res == ConsensusPathPrevalentResult.TOO_FEW_REQUESTS_ON_EXPECTED_PATH:
                                # continue polling for enough right path requests
                                await trio.sleep(seconds=.5)
                                continue
                            if res == ConsensusPathPrevalentResult.TOO_MANY_REQUESTS_ON_UNEXPECTED_PATH:
                                # wait a bit before resending new requests
                                await trio.sleep(seconds=5)

    async def wait_for_last_executed_seq_num(self, replica_id=0, expected=0):
        with log.start_action(action_type="wait_for_last_executed_seq_num"):
            with trio.fail_after(seconds=30):
                while True:
                    with trio.move_on_after(seconds=.5):
                        try:
                            key = ['replica', 'Gauges', 'lastExecutedSeqNum']
                            last_executed_seq_num = await self.metrics.get(replica_id, *key)
                        except KeyError:
                            continue
                        else:
                            # success!
                            if last_executed_seq_num >= expected:
                                return last_executed_seq_num

    async def assert_state_transfer_not_started_all_up_nodes(self, up_replica_ids):
        with log.start_action(action_type="assert_state_transfer_not_started_all_up_nodes"):
            with trio.fail_after(METRICS_TIMEOUT_SEC):
                # Check metrics for all started nodes in parallel
                async with trio.open_nursery() as nursery:
                    up_replicas = [self.replicas[i] for i in up_replica_ids]
                    for r in up_replicas:
                        nursery.start_soon(self._assert_state_transfer_not_started,
                                           r)

    async def assert_fast_path_prevalent(self, nb_fast_paths_to_ignore=0, nb_slow_paths_to_ignore=0, replica_id=0):
        res = await self._consensus_path_prevalent(
            fast=True,
            nb_fast_paths_to_ignore=nb_fast_paths_to_ignore,
            nb_slow_paths_to_ignore=nb_slow_paths_to_ignore,
            replica_id=replica_id)
        assert res == ConsensusPathPrevalentResult.OK, "Fast path is not prevalent"

    async def assert_slow_path_prevalent(self, nb_fast_paths_to_ignore=0, nb_slow_paths_to_ignore=0, replica_id=0):
        res = await self._consensus_path_prevalent(
            fast=False,
            nb_fast_paths_to_ignore=nb_fast_paths_to_ignore,
            nb_slow_paths_to_ignore=nb_slow_paths_to_ignore,
            replica_id=replica_id)
        assert res == ConsensusPathPrevalentResult.OK, "Slow path is not prevalent"

    async def _consensus_path_prevalent(
            self, fast, nb_fast_paths_to_ignore=0, nb_slow_paths_to_ignore=0, replica_id=0):
        """
        Asserts all executed requests after the ignored number have been processed on the fast or slow path
        depending on the fast parameter value being True/False
        """
        with log.start_action(action_type="assert_%s_path_prevalent" % ("fast" if fast else "slow")):
            total_nb_fast_paths = await self.num_of_fast_path_requests(replica_id)
            total_nb_slow_paths = await self.num_of_slow_path_requests(replica_id)

            log.log_message("assert on %s path | slow %d/%d fast %d/%d" % ("fast" if fast else "slow",
                total_nb_slow_paths, nb_slow_paths_to_ignore, total_nb_fast_paths, nb_fast_paths_to_ignore))

            if fast:
                if total_nb_slow_paths > nb_slow_paths_to_ignore:
                    return ConsensusPathPrevalentResult.TOO_MANY_REQUESTS_ON_UNEXPECTED_PATH  # we can't have any slow
                if total_nb_fast_paths <= nb_fast_paths_to_ignore:
                    return ConsensusPathPrevalentResult.TOO_FEW_REQUESTS_ON_EXPECTED_PATH  # we don't have enough fast

                return ConsensusPathPrevalentResult.OK
            else:
                if total_nb_fast_paths > nb_fast_paths_to_ignore:
                    return ConsensusPathPrevalentResult.TOO_MANY_REQUESTS_ON_UNEXPECTED_PATH  # we can't have any fast
                if total_nb_slow_paths <= nb_slow_paths_to_ignore:
                    return ConsensusPathPrevalentResult.TOO_FEW_REQUESTS_ON_EXPECTED_PATH  # we don't have enough slow

                return ConsensusPathPrevalentResult.OK

    async def _assert_state_transfer_not_started(self, replica):
        key = ['replica', 'Counters', 'receivedStateTransferMsgs']
        n = await self.metrics.get(replica.id, *key)
        assert n == 0

    async def wait_for(self, predicate, timeout, interval):
        """
        Wait for the given async predicate function to return true. Give up
        waiting for the async function to complete after interval (seconds) and retry
        until timeout (seconds) expires. Raise trio.TooSlowError when timeout expires.

        Important:
         * The given predicate function must be async
         * Retries may occur more frequently than interval if the predicate
           returns false before interval expires. This only matters in that it
           uses more CPU.
        """
        with log.start_action(action_type="wait_for"):
            with trio.fail_after(timeout):
                while True:
                    with trio.move_on_after(interval):
                        if await predicate():
                            return

    async def num_of_fast_path_requests(self, replica_id=0):
        """
        Returns the total number of requests processed on the fast commit path
        """
        with log.start_action(action_type="num_of_fast_path_requests"):
            with trio.fail_after(seconds=5):
                while True:
                    with trio.move_on_after(seconds=2):
                        try:
                            metric_key = ['replica', 'Counters', 'totalFastPathRequests']
                            nb_fast_path = await self.metrics.get(replica_id, *metric_key)
                            return nb_fast_path
                        except KeyError:
                            # metrics not yet available, continue looping
                            pass

    async def num_of_slow_path_requests(self, replica_id=0):
        """
        Returns the total number of requests processed on the slow commit path
        """
        with log.start_action(action_type="num_of_slow_path_requests"):
            with trio.fail_after(seconds=5):
                while True:
                    with trio.move_on_after(seconds=.5):
                        try:
                            metric_key = ['replica', 'Counters', 'totalSlowPathRequests']
                            nb_slow_path = await self.metrics.get(replica_id, *metric_key)
                            return nb_slow_path
                        except KeyError:
                            # metrics not yet available, continue looping
                            pass
    
    async def do_key_exchange(self):
        """
        Performs initial key exchange, starts all replicas, validate the exchange and stops all replicas.
        The stop is done in order for a test who uses this functionallity, to proceed wihtout imposing n up replicas.
        """
        with log.start_action(action_type="do_key_exchange"):
            self.start_all_replicas()
            with trio.fail_after(seconds=120):
                for replica_id in range(self.config.n):
                    while True:
                        with trio.move_on_after(seconds=1):
                            try:
                                key = ['KeyManager', 'Counters', 'KeyExchangedOnStartCounter']
                                value = await self.metrics.get(replica_id, *key)
                                if value < self.config.n:
                                    continue
                            except trio.TooSlowError:
                                print(
                                    f"Replica {replica_id} was not able to exchange keys on start")
                                raise KeyExchangeError
                            else:
                                assert value == self.config.n
                                break
                                

            with trio.fail_after(seconds=5):
                lastExecutedKey = ['replica', 'Gauges', 'lastExecutedSeqNum']
                lastExecutedVal = await self.metrics.get(0, *lastExecutedKey)
            self.stop_all_replicas()
            return lastExecutedVal
